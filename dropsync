#!/usr/bin/python
#
# dropsync - program to backup data over a synchronized network folder
#
# This program allows you to keep a large data directory synchronized
# with one or more backups, using a synchronized network folder
# (like dropbox).  It does this while not exceeding the file limits
# of the network folder (e.g. 5 gig).  The network folder is used
# for data transfers, but not long-term storage
#
# Copyright 2014,2015 - Tim Bird <tbird20d@yahoo.com>
#
# see bottom of file for data and algorithm information
#
# To do:
#  - create index faster (only calculate sha1 if date, time or size change)
#  - partition large files
#
# CHANGELOG
# 0.7.2 - fix typo in usage
# 0.7.1 - don't use directory size in index.  this fixes a bug where
# 	  the size is different even when the dir contents are identical
#	  (which resulted in never-converging dir entries)
# 0.7.0 - do faster indexing: use previous index as a sha1 cache.
#         Also, do rename on index file creation, and avoid re-reading
# 	  the transfer_file_list execessively during sends
# 0.6.1 - add ending time to log
# 0.6.0 - add support for logging
# 0.5.2 - added transfer_skip_count to show non-transferred files

import sys, os
import sha
import stat
import datetime
import time
#import pdb
#pdb.set_trace()

# version information
major_version = 0
minor_version = 7
revision = 2

debug = 0
verbose = 0
transfer_count = 0
transfer_skip_count = 0

# used to speed up index generation
# has the previous index tree, if index file is present
index_cache = None

def usage():
	prog_name = os.path.basename(sys.argv[0])
	print """Usage: %s [options]

  -h         show this usage
  -r         receive files (default mode is to send)
  -i         initialize %s (for either send or receive)
  -v         be verbose
  -c <file>  specify configuration file (default is '/etc/sda.conf')
               if no config file is found, test settings are used
  -l <file>  log operations to the specified file
  --version  show version information, and exit
""" % (prog_name, prog_name)

# create a class for objects with arbitrary attributes
class stub_class:
	pass

# these items should be configured by the user
# but put in some defaults for testing
default_config = stub_class()

test_dir = "/home/tbird/work/dropsync/test/"
default_config.source_dir = test_dir+"source_dir"
default_config.dest_dir = test_dir+"dest_dir"
default_config.sync_dir = test_dir+"sync_dir"
# Transfer limits should match any restrictions on
# your synchronization directory
default_config.transfer_size_limit = 1000000000
default_config.transfer_count_limit = 1000

# configuration file syntax:
# ------------------------
# empty lines not in a block are ignored
# lines starting with # are ignored
# single-line attributes are specified with '=', like so:
# name=value
# multi-line attributes use triple-quotes, like so:
# name="""value line 1
# value line 2, etc."""
#
def read_config_file(config_path):
	# look in configuration directory
	info_file = os.path.basename(config_path)
	try:
		fl = open(config_path)
	except:
		error_out("Cannot open configuration file %s" % config_path, 3)

	valid_attr_list = { 
		"source_dir":'s',	# directory to copy files from
		"dest_dir":'s',	# directory to copy files to
		"sync_dir":'s',	# directory to use for transfers
		"transfer_size_limit":'d',	# limit, in bytes, of transfer files
		"transfer_count_limit":'d',	# max number of transfer files
	}	
	config = stub_class()
	in_block = 0
	block = ""
	line_no = 0
	for line in fl.readlines():
		line_no += 1
		if line.startswith("#"):
			continue
		if in_block:
			# try to find end of block
			if line.rstrip().endswith('"""'):
				# remove quotes and end block
				line = line.rstrip()
				value = block + line[:-3] + "\n"
				setattr(config, name, value)
				in_block = 0
				continue
			else:
				block += line
				continue

		# OK, it's not a comment or middle of a block.
		# check if it's empty
		if not line.strip():
			continue

		# line should have an equals in it
		# (either single line name=value, or multi-line block start)
		if line.find("=")==-1:
			aprint("Syntax error in section info file %s: Expected '=' at line %d:\n%s" % (info_file, line_no, line))
			continue
		
		(name, value) = line.split('=', 1)
		name = name.strip()
		if name not in valid_attr_list.keys():
			aprint("WARNING: '%s' is not a recognized configuration option" % name)
			aprint("at line %d in config file: %s" % (line_no, info_file))
			
		value = value.strip()
		if value.find('"""')==-1:
			# this is a single-line, just record the attribute
			try:
				attr_type = valid_attr_list[name]
			except:
				attr_type = 's'
			# convert items to integers, if type specifies this
			if attr_type == 'd':
				try:
					value = int(value) 
				except:
					aprint("ERROR: expected integer value for option '%s'" % name)
					sys.exit(1)
			setattr(config, name, value)
		else:
			# this is the start of a multi-line block
			vstart = value.find('"""')
			block = value[vstart+3:] + '\n'
			in_block = 1
			# sanity check for block terminator on same line
			# if triple-quotes end this line, then block begins
			# and ends on the same line.
			if block.endswith('"""\n'):
				value = block[:-3]
				setattr(config, name, value)
				in_block = 0

	return config

# search for config file, using the following priority:
#  one specified on command line
#  one it /etc (system-wide)
#  one in ~/ (user-specific)
# if none found, return ""
def find_config_file(user_specified_config_file):
	# FIXTHIS - should check for config file readable, in find_config_file
	if user_specified_config_file:
		# check that file exists
		if not os.path.isfile(user_specified_config_file):
			aprint("ERROR: config file '%s' not found" % \
				user_specified_config_file)
			aprint("Exiting.")
			sys.exit(1)
		else:
			return user_specified_config_file

	# check for system sda.conf file
	if os.path.isfile("/etc/sda.conf"):
		return "/etc/sda.conf"

	# check for user-specific ~/.sda.conf file
	home_dir = ""
	try:
		home_dir = os.environ["HOME"]
	except:
		pass

	if home_dir and os.path.isfile(home_dir+os.path.sep+".sda.conf"):
		return home_dir+os.path.sep+".sda.conf"

	# nothing found...
	return ""

def get_config(default_config, config_path):
	if config_path:
		config = read_config_file(config_path)
	else:
		config = default_config
		aprint("WARNING: using test config settings....")
		aprint("   please specify a config file using -c, or place")
		aprint("   a config file in '/etc' or your home directory.")

	# NOTE - source index does not have to be in the sync dir
	# but it makes it easier to debug the script.
	# source_index should NOT be inside the source_dir, however
	config.source_index = config.sync_dir+os.path.sep+"source_index"
	config.dest_index = config.sync_dir+os.path.sep+"dest_index"
	config.transfer_prefix = "transfer_file_"
	config.transfer_format = config.sync_dir+os.path.sep+ \
		config.transfer_prefix+"%05d"

	# directories should be absolute paths
	config.source_dir = os.path.abspath(config.source_dir)
	config.dest_dir = os.path.abspath(config.dest_dir)
	
	return config

# mode should be "send" or "receive"
def validate_config(config, mode):
	dprint("in validate_config(): config=%s" % config.__dict__)

	# do some sanity checks on the config
	if not os.path.isdir(config.sync_dir):
		aprint("ERROR: sync dir '%s' is not a directory" \
			 % config.sync_dir)
		aprint("Exiting.")
		sys.exit(1)

	if mode == "send":
		if not os.path.isdir(config.source_dir):
			aprint("ERROR: source dir '%s' is not a directory" \
				 % config.source_dir)
			aprint("Exiting.")
			sys.exit(1)

	if mode == "receive":
		if not os.path.isdir(config.dest_dir):
			aprint("ERROR: source dir '%s' is not a directory" \
				 % config.dest_dir)
			aprint("Exiting.")
			sys.exit(1)


# define some classes for sync entries
# entries are generated either by scanning or by parsing
# when parsing, fields are set from strings in index entries
# when scanning, fields are set from file system and sha1 operations
class file_entry:
	def __init__(self, path, size, date, perms, base_path):
		# path is stored in filesystem (unescaped) format
		# path is relative to the base_path (not an absolute path)
		self.path = path
		self.size = size
		self.date = date
		self.perms = perms
		self.sha1 = "<none>"
		self.base_path = base_path
	
	def full_path(self):
		return self.base_path + os.path.sep + self.path

	def escape_path(self, path):
		# escape sequences are:
		# % -> %25
		# , -> %2C
		new1 = path.replace("%", "%25")
		new2 = new1.replace(",", "%2C")
		return new2

	def unescape_path(self, path):
		# back these out in reverse order of application
		new1 = path.replace("%2C", ",")
		new2 = new1.replace("%25", "%")
		return new2

	def mtime(self):
		# returns time in seconds since epoch
		(date_part,time_part) = self.date.split("_")
		year_str, month_str, day_str = date_part.split("-")
		hour_str, minute_str, second_str = time_part.split(":")
		d = datetime.datetime(int(year_str), int(month_str),
			int(day_str), int(hour_str), int(minute_str),
			int(second_str))
		return time.mktime(d.timetuple())

	def calculate_sha1(self):
		global index_cache

		# check the cache first
		if index_cache:
			prev = index_cache.lookup_by_path(self)
			if prev and self.size == prev.size and \
				self.date == prev.date and \
				self.perms == prev.perms:
				dprint("item in cache, using previous sha1 of %s" % \
					prev.sha1)	
				self.sha1 = prev.sha1
			return self.sha1

		# for a file, the sha1 is the sha1 of the file content
		f = open(self.full_path(), mode = 'rb')
		d = sha.new()
		buf = 0
		while buf != b'':
			buf = f.read(32768)
			d.update(buf)
		f.close()
		self.sha1 = d.hexdigest()
		return self.sha1

	def base_line(self):
		epath = self.escape_path(self.path)
		if self.sha1 == "<none>":
			aprint("ERROR: sha1 not calculated yet!")
		return "%s, %d, %s, %s, %s" % (epath, self.size,
			self.date, self.perms, self.sha1)

	def to_line(self):
		return "f, "+self.base_line()

	def is_dir(self):
		return False

	def lookup_by_path(self, entry):
		if self.path == entry.path:
			return self
		else:
			return None

	def attr_match(self, entry):
		dprint("self.sha1=%s" % self.sha1)
		dprint("entry.sha1=%s" % entry.sha1)
		dprint("self.size=%d" % self.size)
		dprint("entry.size=%d" % entry.size)
		dprint("self.date=%s" % self.date)
		dprint("entry.date=%s" % entry.date)
		dprint("self.perms=%s" % self.perms)
		dprint("entry.perms=%s" % entry.perms)
		if self.sha1 == entry.sha1 and \
			self.size == entry.size and \
			self.date == entry.date and \
			self.perms == entry.perms:
			dprint("matched!")
			return self
		else:
			dprint("did not match!")
			return None
	def __repr__(self):
		return "entry %s" % os.path.basename(self.path)


class dir_entry(file_entry):
	def __init__(self, path, size, date, perms, base_path):
		# path is stored in filesystem (unescaped) format
		# path is relative to the base_path (not an absolute path)
		file_entry.__init__(self, path, size, date, perms, base_path)
		self.sub_entries = []

	def add_sub_entry(self, entry):
		self.sub_entries.append(entry)

	def sort_sub_entries(self):
		entry_list = sorted(self.sub_entries,
			key = lambda entry: entry.path)
		self.sub_entries = entry_list

	def calculate_sha1(self):
		# sha1 for a directory is the digest of sub-entry strings
		# sorted by path
		self.sort_sub_entries()
		temp = ""
		for entry in self.sub_entries:
			temp += entry.to_line()+"\n"

		# now generate the digest from the buffer
		self.sha1 = sha.new(temp).hexdigest()
		return self.sha1

	def to_line(self):
		return "d, "+self.base_line()

	def is_dir(self):
		return True

	def lookup_by_path(self, entry):
		dprint("in (dir_entry).lookup_by_path()")
		dprint("self=%s" % self)
		dprint("self.sub_entries=%s" % self.sub_entries)
		dprint("entry=%s" % entry)
		if self.path == entry.path:
			return self
		else:
			found = None
			for child in self.sub_entries:
				if entry.path.startswith(child.path):
					found = child.lookup_by_path(entry)
					if found:
						break
			return found


class index_class:
	def __init__(self, sequence, root):
		self.sequence = sequence
		self.root = root

def parse_index_entry(line, base_path):
	parts = line.split(", ")
	if len(parts) != 6:
		aprint("!!! ERROR - problem extracting index data")
		aprint("line was: '%s'" % line)
	entry_type = parts[0]
	path = parts[1]
	size = int(parts[2])
	date = parts[3]
	perms = parts[4]
	sha1 = parts[5]
	
	if entry_type == "f":
		e = file_entry(path, size, date, perms, base_path)
	else:
		e = dir_entry(path, size, date, perms, base_path)
	e.path = e.unescape_path(path)
	e.sha1 = sha1
	return e

# here are some routines for logging
logf = None

def open_log(log_path, mode, gen_index):
	global logf
	import atexit

	logf = open(log_path, "a")
	date_str = time.ctime()
	write_log("=========================================================")
	write_log("Log for dropsync execution at: %s" % date_str)
	write_log("mode=%s, gen_index=%s" % (mode, gen_index))
	write_log("=========================================================")

	atexit.register(close_log)

def write_config_to_log(config):
	write_log("config:")
	write_log("  source_dir=%s" % config.source_dir)
	write_log("  dest_dir=%s" % config.dest_dir)
	write_log("  sync_dir=%s" % config.sync_dir)
	write_log("  transfer_size_limit=%s" % config.transfer_size_limit)
	write_log("  transfer_count_limit=%s" % config.transfer_count_limit)
	write_log("=========================================================")

def write_log(msg):
	global logf

	if logf:
		logf.write(msg+"\n")

def close_log():
	global logf

	dprint("Closing log.")
	if logf:
		date_str = time.ctime()
		write_log("Ending execution at: %s" % date_str)
		write_log("=========================================================")
		logf.close()
		logf = None

# always print (and log)
def aprint(msg):
	print msg
	write_log(msg)

# debug print
def dprint(msg):
	if debug:
		aprint(msg)

# verbose print
def vprint(msg):
	if verbose:
		print msg

	# logs always get verbose messages
	write_log(msg)

def perm_to_ascii(mode):
	return "perm-not-done"

def relative_path(full_path, base_path):
	# trim path to be relative to base_path
	if not full_path.startswith(base_path):
		aprint("WARNING: path '%s' is not inside base path of '%s'" \
			% (full_path, base_path))
		aprint("cannot create relative path")
		return full_path
	
	rel_path = full_path[len(base_path)+1:]
	return rel_path

def create_file_entry(full_path, base_path):
	st = os.stat(full_path)

	rel_path = relative_path(full_path, base_path)
	
	size = st.st_size
	# FIXTHIS - should this be ctime or mtime?
	date = datetime.datetime.fromtimestamp(st.st_mtime)
	date_str = date.strftime("%Y-%m-%d_%H:%M:%S")
	perm_str = perm_to_ascii(stat.S_IMODE(st.st_mode))

	entry = file_entry(rel_path, size, date_str, perm_str, base_path)
	entry.calculate_sha1()
	return entry

def create_dir_entry(full_path, base_path):
	st = os.stat(full_path)

	rel_path = relative_path(full_path, base_path)

  	# Ignore size for dirs.
	# size may be different, even for directories with identical contents.
	# IDEA: could use count of contents as size instead in create_dir_entry
	size = 0

	date = datetime.datetime.fromtimestamp(st.st_mtime)
	date_str = date.strftime("%Y-%m-%d_%H:%M:%S")
	perm_str = perm_to_ascii(stat.S_IMODE(st.st_mode))

	entry = dir_entry(rel_path, size, date_str, perm_str, base_path)
	return entry

def process_path(cur_path, base_path):
	dprint("Processing path '%s'" % cur_path)
	full_path = os.path.abspath(cur_path)

	# if it's a regular file, just create the node
	if os.path.isfile(full_path):
		entry = create_file_entry(full_path, base_path)
		return entry
	
	# if a directory, create the node and populate children before
	# calculating the sha1
	if os.path.isdir(full_path):
		entry = create_dir_entry(full_path, base_path)

		item_list = os.listdir(full_path)

		for item in item_list:
			item_path = os.path.join(full_path, item)
			
			child = process_path(item_path, base_path)
			if child:
				entry.add_sub_entry(child)

		entry.calculate_sha1()
		return entry
			
	# skip things that are not files or directories
	# (symlinks, device files, etc.)
	return None

def print_tree(entry):
	aprint(entry.to_line())
	if entry.is_dir():
		# NOTE: sub_entries should already be sorted
		for child in entry.sub_entries:
			print_tree(child)

def write_tree(f, entry):
	f.write(entry.to_line()+"\n")
	if entry.is_dir():
		# NOTE: sub_entries should already be sorted
		for child in entry.sub_entries:
			write_tree(f,child)

def write_index(filename, index):
	f = open(filename, "w")

	seq_line = "%040d\n" % index.sequence
	f.write(seq_line)

	# now write the tree
	write_tree(f, index.root)
	f.close()
		
def generate_index(filename, dir_to_be_indexed):
	# scan directory, generating index lines
	entry = process_path(dir_to_be_indexed, dir_to_be_indexed)

	# use starting sequence of 0000...01
	index = index_class(1, entry)

	write_index(filename, index)


# returns an index object for the root of the tree
def read_index(filename, base_path):
	f = open(filename, "r")

	seq_line = f.readline()
	sequence = int(seq_line)

	root = None
	dir_stack = []
	for line in f.xreadlines():
		# parse line with trailing whitespace ('\n') removed
		entry = parse_index_entry(line.rstrip(), base_path)
		dprint("processing entry %s" % entry.path)
		dprint("dir_stack=%s" % dir_stack)

		if not root:
			# this only happens once
			dir_stack.append(entry)
			root = entry
			continue

		# manage dir_stack
		# pop items that are not my parent directories
		while not entry.path.startswith(dir_stack[-1].path):
			dir_stack.pop()

		# put me on my parent dir's child list
		if entry.path.startswith(dir_stack[-1].path):
			dir_stack[-1].add_sub_entry(entry)
		else:
			aprint("What happened?, parent dir should be on dir_stack")
			aprint("entry.path=%s" % entry.path)
			aprint("dir_stack[-1].path=%s" % dir_stack[-1].path)

		# if I'm a directory, put me on dir_stack
		if entry.is_dir():
			dir_stack.append(entry)
	
	return index_class(sequence, root)

def update_index(index_path, dir_to_be_indexed):
	new_index_path = index_path+".new"
	# if file doesn't exist, create it
	if not os.path.isfile(index_path):
		index_cache = None
	else:
		# read the old index to use as a sha1 cache
		index_cache = read_index(index_path, dir_to_be_indexed)

	generate_index(new_index_path, dir_to_be_indexed)
	index_cache = None

	try:
		os.rename(new_index_path, index_path)
	except:
		# on Windows, if the file exists, the rename may give
		# an exception.  In this case, fall back to remove, then rename
		try:
			os.remove(index_path)
			os.rename(new_index_path, index_path)
		except:
			aprint("ERROR trying to atomically rename index file")
			aprint("Can't rename '%s' to '%s'" % \
				(new_index_path, index_path))
	

def get_transfer_file_list():
	global config

	# get the transfer file info from config.
	# if not found, we haven't scanned for it.  Do that now.
	try:
		tfiles = config.transfer_file_list
	except:
		# scan sync_dir looking for transfer files
		file_list = os.listdir(config.sync_dir)

		tfiles = []
		for filename in file_list:
			if filename.startswith(config.transfer_prefix):
				tfiles.append(config.sync_dir+os.path.sep+filename)
		config.transfer_file_list = tfiles

	dprint("transfer files=%s" % tfiles)
	return tfiles

def scan_for_max_transfer_num():
	tfiles = get_transfer_file_list()

	# get largest number used so far
	max_num = 0
	for filename in tfiles:
		try:
			end_num = int(filename[-5:])
			if end_num > max_num:
				max_num = end_num
		except:
			pass

	return max_num

def get_next_transfer_filename():
	global config

	# get the max transfer number from config.
	# if not found, we haven't scanned for it.  Do that now.
	try:
		max_num = config.max_transfer_num
	except:
		max_num = scan_for_max_transfer_num()

	max_num += 1
	config.max_transfer_num = max_num
	filename = config.transfer_format % max_num
	dprint("next transfer filename='%s'" % filename)
	return filename
		

# only called for files, not directories
# transfer_type must be one of: "remove" or "update"
def make_transfer_file(entry, transfer_type):
	global transfer_count

	filename = get_next_transfer_filename()
	vprint("Creating transfer file '%s' for file '%s'" % \
		(filename, entry.path))

	outfile = open(filename, "w")
	config.transfer_file_list.append(filename)

	if transfer_type == "update":
		outfile.write(entry.to_line()+"\n")
	else:
		# this is a "remove" transfer
		saved_sha1 = entry.sha1
		entry.sha1 = "<deleted>"
		outfile.write(entry.to_line()+"\n")
		entry.sha1 = saved_sha1
		outfile.close()
		config.transfer_file_total_size += os.path.getsize(filename)
		transfer_count += 1
		return

	if not entry.is_dir():
		# put file content after entry line
		infile = open(entry.full_path(), "rb")
		buf = 0
		while buf != b'':
			buf = infile.read(32768)
			outfile.write(buf)
		infile.close()

	outfile.close()
	transfer_count += 1
	config.transfer_file_total_size += os.path.getsize(filename)

def get_current_transfer_size():
	try:
		total_size = config.transfer_file_total_size
	except:
		# scan list of transfer files, and sum their sizes
		tfiles = get_transfer_file_list()

		total_size = 0
		for filename in tfiles:
			total_size += os.path.getsize(filename)
		config.transfer_file_total_size = total_size

	dprint("current total transfer size=%d" % total_size)
	return total_size

def get_current_transfer_count():
	# count the list of transfer files
	tfiles = get_transfer_file_list()

	count = len(tfiles)
	dprint("current transfer count=%d" % count)
	return count


def within_transfer_limit(entry):
	global config
	global transfer_skip_count

	# check count and size limits for transfer directory
	if get_current_transfer_count() >= config.transfer_count_limit:
		vprint("transfer count limit reached, not sending '%s' this time" % \
			entry.path)
		transfer_skip_count += 1
		return False
	elif get_current_transfer_size() >= config.transfer_size_limit:
		vprint("transfer size limit reached, not sending '%s' this time" % \
			entry.path)
		transfer_skip_count += 1
		return False
	else:
		return True

######################## old action system ############################
def action_send(entry):
	dprint("in action_send for '%s'" % entry)

	if not within_transfer_limit(entry):
		return

	make_transfer_file(entry, "update")

def action_remove(entry):
	dprint("in action_remove for '%s'" % entry)
	if not within_transfer_limit(entry):
		return

	if not entry.is_dir():
		make_transfer_file(entry, "remove")
		return
	else:
		for child in entry.sub_entries:
			make_transfer_file(child, "remove")
		make_transfer_file(entry, "remove")

def walk_tree(entry, other_tree, action):
	# look up entry in other tree, if different, do callback
	dprint("walking tree at entry %s" % entry)
	other = other_tree.lookup_by_path(entry)
	dprint("other= %s" % other)
	if other == None or not other.attr_match(entry):
		action(entry)

	if entry.is_dir():
		dprint("%s is a dir, walking children" % entry.path)
		for child in entry.sub_entries:
			walk_tree(child, other_tree, action)

######################## new action system ############################
def walk_tree_for_updates(entry, other_tree):
	# look up entry in other tree, if different, do callback
	dprint("walking tree at entry %s" % entry)
	other = other_tree.lookup_by_path(entry)
	dprint("other= %s" % other)

	# if item is not found, or is different in other tree, update it
	if other == None or not other.attr_match(entry):
		if within_transfer_limit(entry):
			make_transfer_file(entry, "update")

	if entry.is_dir():
		dprint("%s is a dir, walking children" % entry.path)
		for child in entry.sub_entries:
			walk_tree_for_updates(child, other_tree)

def walk_tree_for_removes(entry, other_tree):
	# look up entry in other tree, if different, do callback
	dprint("walking tree at entry %s" % entry)
	other = other_tree.lookup_by_path(entry)
	dprint("other= %s" % other)

	# if item is not present in other tree, remove it
	if other == None:
		if within_transfer_limit(entry):
			make_transfer_file(entry, "remove")

	if entry.is_dir():
		dprint("%s is a dir, walking children" % entry.path)
		for child in entry.sub_entries:
			walk_tree_for_removes(child, other_tree)

def do_send():
	global config

	# check that dest_index is present
	if not os.path.isfile(config.dest_index):
		prog_name = os.path.basename(sys.argv[0])
		aprint("ERROR: could not find dest_index: '%s'" \
			% config.dest_index)
		aprint("Cannot continue in send mode.")
		aprint("Create dest_index file by running '%s -r' on the destionation host" % prog_name)
		aprint("Exiting.")
		sys.exit(1)

	# for DEBUGGING - receiver can leave transfer files around
	# get rid of them before starting
#	tfiles = get_transfer_file_list()
#	for filename in tfiles:
#		os.remove(filename)

	vprint("Updating source index")
	update_index(config.source_index, config.source_dir)

	# read the source and destination indexes
	vprint("Reading source index")
	si = read_index(config.source_index, config.source_dir)

	vprint("Reading destination index")
	di = read_index(config.dest_index, config.dest_dir)
	
	# scan the indexes, and print a message for each difference
	# do a traversal, and look up the item in the other tree
	vprint("Performing update actions")
	walk_tree_for_updates(si.root, di.root)
	
	vprint("Performing removal actions")
	walk_tree_for_removes(di.root, si.root)

def process_transfer_file(filename):
	global transfer_count

	# read transfer file and act on it
	f = open(filename, "rb")
	line = f.readline()
	entry = parse_index_entry(line.rstrip(), config.dest_dir)

	vprint("Processing transfer file %s, for path '%s'" \
		% (filename[-5:], entry.path))

	# NOTE - transfer file is still open at this point

	if entry.sha1 == "<deleted>":
		f.close()
		if entry.is_dir():
			vprint("Deleting directory")
			try:
				os.rmdir(entry.full_path())
			except:
				pass
			dprint("os.rmdir('%s')" % entry.path)
		else:
			vprint("Deleting file")
			try:
				os.remove(entry.full_path())
			except:
				pass
			dprint("os.remove('%s')" % entry.path)
		dprint("Removing transfer file")
		os.remove(filename)
		transfer_count += 1
		return

	# not a remove, make something
	if entry.is_dir():
		# make directory, if not already present
		vprint("Creating directory")
		f.close()
		try:
			os.makedirs(entry.full_path())
		except:
			pass
	else:
		vprint("Copying file contents")
		# create leading dirs in destination dir if needed
		try:
			os.makedirs(os.path.dirname(entry.full_path()))
		except:
			pass

		# copy the file contents, from remainder of transfer file
		outfile = open(entry.full_path(), "wb")
		buf = 0
		while buf != b'':
			buf = f.read(32768)
			outfile.write(buf)

		outfile.close()
		f.close()

	# touch the file date and time
	mtime = entry.mtime()
	os.utime(entry.full_path(), (mtime,mtime))
	
	# FIXTHIS - adjust permissions

	dprint("Removing transfer file")
	os.remove(filename)

	transfer_count += 1

def do_receive():
	global config

	# process any transfer files
	tfiles = get_transfer_file_list()

	for filename in tfiles:
		process_transfer_file(filename)

	# update destination index
	vprint("Updating dest_index file")
	update_index(config.dest_index, config.dest_dir)

def main():
	global debug
	global verbose
	global default_config
	global config
	global log_file
	global index_cache

	mode = "send"
	do_gen_index = False
	user_specified_config_file = ""
	log_file = None

	if "-r" in sys.argv:
		sys.argv.remove("-r")
		mode = "receive"

	if "-h" in sys.argv or "--help" in sys.argv:
		usage()
		sys.exit(0)

	if "-v" in sys.argv:
		sys.argv.remove("-v")
		verbose = 1
	if "--debug" in sys.argv:
		sys.argv.remove("--debug")
		debug = 1

	if "-i" in sys.argv:
		sys.argv.remove("-i")
		do_gen_index = True

	if "-c" in sys.argv:
		index = sys.argv.index("-c")
		user_specified_config_file = sys.argv[index+1]
		vprint("Using config file: %s" % user_specified_config_file)
		del sys.argv[index+1]
		del sys.argv[index]

	if "-l" in sys.argv:
		index = sys.argv.index("-l")
		log_file = sys.argv[index+1]
		del sys.argv[index+1]
		del sys.argv[index]
	
	if "--version" in sys.argv:
		prog_name = os.path.basename(sys.argv[0])
		aprint("%s version %d.%d.%d" % \
			(prog_name, major_version, minor_version, revision))
		sys.exit(0)

	for arg in sys.argv[1:]:
		aprint("WARNING: unknown option '%s' was specified." % arg)
		aprint("  option is being ignored")

	if log_file:
		vprint("Using log file: %s" % log_file)
		open_log(log_file, mode, do_gen_index)

	# find, read and validate the configuration
	config_path = find_config_file(user_specified_config_file)
	config = get_config(default_config, config_path)
	validate_config(config, mode)

	write_config_to_log(config)

	if do_gen_index:
		# remove transfer files
		for filename in get_transfer_file_list():
			os.remove(filename)

		# generate initial index
		index_cache = None
		if mode=="send":
			vprint("Generating source index...")
			generate_index(config.source_index, config.source_dir)
		else:
			vprint("Generating destination index...")
			generate_index(config.dest_index, config.dest_dir)

		sys.exit(0)
		
	if mode == "send":
		do_send()
		aprint("Created %d transfer files." % transfer_count)
		if transfer_skip_count:
			aprint("Skipped %d items that still need to be transferred." % transfer_skip_count)
	else:
		do_receive()
		aprint("Processed %d transfer files." % transfer_count)


	vprint("Done.")
	

if __name__ == "__main__":
	main()

# === Information about the data and algorithms for this program ===
# 
# the source machine has:
#   source_dir - the location of files to be backed up
#   source_index - the index of source_dir
#   sync_dir - the directory for transferring synced files
#     dest_index - the index of dest_dir
#     transfer_file_N - a file being transferred to the destination
#
# the destination machine has:
#   sync_dir - the directory for transferring synced files
#     dest_index - the index of dest_dir
#     transfer_file_N - a file being transferred to the destination
#   dest_dir - the location of backup files on the destination
#
# == sender algorithm ==
# each time run, do:
# update source_index
#   if source_index is not present, generate it
#   else scan source_dir, and update source_index
# transfer files to update
#   compare source_index with dest_index
#   if file is missing or different in dest_index
#     create transfer file in d1
#     loop until size would be exceeded or no more files to transfer
# check that file transfered
#   watch dest_index for change 
#   scan dest_index
#     if entry for transfer file matches source, remove transfer file
# loop until there are no more files to transfer
#
# == receiver algorithm ==
# each time run, do:
# update dest_index
#   if dest_index is not present, generate it
#   else scan dest_dir, and update dest_index
# receive transfer files
#   if there's a transfer file,
#     [check if file in dest_dir matches or not] (optional optimization)
#     copy transfer file to dest_dir
#     update dest_index with entry for newly copied file
#       get sha1, rewrite dest_index
#     loop no more files to transfer
#
# each index entry is one line with the format
# path, size, date, perms, sha1
# if path has commas, they are escaped as follows:
#  \\ = \
#  \, = ,
# date is of format: 2014-12-31_12:59:59
# perms are unix perms in string format

