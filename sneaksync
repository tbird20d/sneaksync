#!/usr/bin/python
#
# sneaksync - program to backup data over a synchronized network folder
#
# This program allows you to keep a large data directory synchronized
# with one or more backups, using a synchronized network folder
# (like dropbox).  It does this while not exceeding the file limits
# of the network folder (e.g. 2 gig).  The network folder is used
# for data transfers, but not long-term storage
#
# Copyright 2014,2015 - Tim Bird <tbird20d@yahoo.com>
#
# See bottom of file for data and algorithm information
#
# GPL version 2.0 applies.  See the LICENSE file for more information.
#
# To do:
#  - on sender wakeup, detect source directory change
#
# CHANGELOG
# 1.0.3 - handle several file system exception conditions
# 1.0.2 - open index file in binary mode so Windows doesn't add \r on lines
# 1.0.1 - use posix path separator in index and transfer files, even on
#   Windows (all other paths use local separator)
# 1.0.0 - rename to sneaksync.  I'm not totally happy with it, as 'sneak'
#   has a somewhat negative connotation.  But dropsync is not available,
#   and I don't have something better.
# 0.9.9 - show activity on long operations (index creation)
# 0.9.8 - refactor classes, fix resulting bugs in part file transfers
# 0.9.7 - change recalc_list handling, add parents to index tree
# 0.9.6 - refactor global data items into a class
# 0.9.5 - in period mode, check for activity from other side before looping
# 0.9.4 - fix write to last_offset, and ignore file limits on removes
# 0.9.3 - numerous fixes for part files: generate part file at destination,
#  and use it to trigger recalculating the sha1 of the base file,
#  truncate files in case they have shrunk
# 0.9.2 - don't send files bigger than part_size (only their parts)
# 0.9.1 - fix bug with config missing an exclude_list
# 0.9.0 - support partitions
# 0.8.5 - support exclude_list in config, for omitting paths by re pattern
# 0.8.0 - support -p option to cause program to loop
# 0.7.4 - fix problem with <none> sha1 entries in index
# 0.7.3 - fix index_cache usage, and move log routines
# 0.7.2 - fix typo in usage
# 0.7.1 - don't use directory size in index.  this fixes a bug where
# 	  the size is different even when the dir contents are identical
#	  (which resulted in never-converging dir entries)
# 0.7.0 - do faster indexing: use previous index as a sha1 cache.
#         Also, use os.rename for index file creation, and avoid re-reading
# 	  the transfer_file_list execessively during sends
# 0.6.1 - add ending time to log
# 0.6.0 - add support for logging
# 0.5.2 - added transfer_skip_count to show non-transferred files

import sys, os
import sha
import stat
import datetime
import time
import re
#import pdb
#pdb.set_trace()

# version information
major_version = 1
minor_version = 0
revision = 3 

debug = 0
verbose = 0

# path separator - don't use os.path.sep for filenames in index and 
# transfer files.  The separators have to be the same to allow
# interoperability between platforms (eg syncing from Windows to Linux).
# Windows takes both "\" and "/", and Linux only
# takes "/", so use that for index and transfer files.

# base_line(), to_line() and dir_entry.calculate_sha1() use posix_sep
# all other paths use local separators
posix_sep = "/"

# a cache is used to speed up index generation
# has the previous index tree, if index file is present
index_cache = None

def usage():
	prog_name = os.path.basename(sys.argv[0])
	print """Usage: %s [options]

  -h         show this usage
  -r         receive files (default mode is to send)
  -i         initialize %s (for either send or receive)
  -v         be verbose
  -vv        be very verbose
  -c <file>  specify configuration file (default is '/etc/sneaksync.conf')
               if no config file is found, test settings are used
  -l <file>  log operations to the specified file
  --version  show version information, and exit
  -p <period> run the program in a loop, and poll for changes every
	      <period> minutes
  -a         show activity status while running
  -x <pat>   exclude directories or files matching the indicated re pattern
             This option may be specified multiple times
""" % (prog_name, prog_name)

# create a class for the configuration
class config_class:
	def __init__(self):
		self.exclude_list = ""
		self.exclude_re_list = []
		# default part_size to 600M
		self.part_size = 600000000

class data_class:
	def __init__(self):
		self.transfer_count = 0
		self.transfer_skip_count = 0
		self.transfer_file_total_size = 0
		self.transfer_file_list_scanned = False
		self.transfer_file_list = []
		# -1 means it hasn't been calculated yet
		self.max_transfer_num = -1
		self.recalc_list = []

# these items should be configured by the user
# but put in some defaults for testing
default_config = config_class()

test_dir = "/home/tbird/work/sneaksync/test/"
default_config.source_dir = test_dir+"source_dir"
default_config.dest_dir = test_dir+"dest_dir"
default_config.sync_dir = test_dir+"sync_dir"
# Transfer limits should match any restrictions on
# your synchronization directory
default_config.transfer_size_limit = 1000000000
default_config.transfer_count_limit = 1000

# here are some routines for logging and printing
logf = None

def open_log(log_path, mode, gen_index):
	global logf
	import atexit

	logf = open(log_path, "a")
	date_str = time.ctime()
	write_log("=========================================================")
	write_log("Log for sneaksync execution at: %s" % date_str)
	write_log("mode=%s, gen_index=%s" % (mode, gen_index))
	write_log("=========================================================")

	atexit.register(close_log)

def write_config_header_to_log(config, mode):
	write_log("config:")
	if mode=="send":
		write_log("  source_dir=%s" % config.source_dir)
	else:
		write_log("  dest_dir=%s" % config.dest_dir)
	write_log("  sync_dir=%s" % config.sync_dir)
	if mode=="send":
		write_log("  transfer_size_limit=%s" %
			config.transfer_size_limit)
		write_log("  transfer_count_limit=%s" %
			config.transfer_count_limit)
	write_log("  part_size=%s" % config.part_size)
	write_log("=========================================================")

def write_log(msg):
	global logf

	if logf:
		logf.write(msg+"\n")
		logf.flush()

def close_log():
	global logf

	dprint("Closing log.")
	if logf:
		date_str = time.ctime()
		write_log("Ending execution at: %s" % date_str)
		write_log("=========================================================")
		logf.close()
		logf = None

# always print (and log)
def aprint(msg):
	print msg
	write_log(msg)

# debug print
def dprint(msg):
	if debug:
		aprint(msg)

# verbose print
def vprint(msg):
	if verbose==1:
		print msg

	# logs always get verbose messages
	write_log(msg)

# very verbose print
def vvprint(msg):
	if verbose:
		print msg

	# logs also get very verbose messages
	write_log(msg)


# configuration file syntax:
# ------------------------
# empty lines not in a block are ignored
# lines starting with # are ignored
# single-line attributes are specified with '=', like so:
# name=value
# multi-line attributes use triple-quotes, like so:
# name="""value line 1
# value line 2, etc."""
#
def read_config_file(config_path):
	# look in configuration directory
	info_file = os.path.basename(config_path)
	try:
		fl = open(config_path)
	except:
		error_out("Cannot open configuration file %s" % config_path, 3)

	valid_attr_list = { 
		"source_dir":'s',	# directory to copy files from
		"dest_dir":'s',	# directory to copy files to
		"sync_dir":'s',	# directory to use for transfers
		"transfer_size_limit":'d',	# limit, in bytes, of transfer files
		"transfer_count_limit":'d',	# max number of transfer files
		"exclude_list":'s',	# list of regex patterns to exclude from transfer
		"part_size":'d',	# size of file parts
	}	
	config = config_class()
	in_block = 0
	block = ""
	line_no = 0
	for line in fl.readlines():
		line_no += 1
		if line.startswith("#"):
			continue
		if in_block:
			# try to find end of block
			if line.rstrip().endswith('"""'):
				# remove quotes and end block
				line = line.rstrip()
				value = block + line[:-3] + "\n"
				setattr(config, name, value)
				in_block = 0
				continue
			else:
				block += line
				continue

		# OK, it's not a comment or middle of a block.
		# check if it's empty
		if not line.strip():
			continue

		# line should have an equals in it
		# (either single line name=value, or multi-line block start)
		if line.find("=")==-1:
			aprint("Syntax error in section info file %s: Expected '=' at line %d:\n%s" % (info_file, line_no, line))
			continue
		
		(name, value) = line.split('=', 1)
		name = name.strip()
		if name not in valid_attr_list.keys():
			aprint("WARNING: '%s' is not a recognized configuration option" % name)
			aprint("at line %d in config file: %s" % (line_no, info_file))
			
		value = value.strip()
		if value.find('"""')==-1:
			# this is a single-line, just record the attribute
			try:
				attr_type = valid_attr_list[name]
			except:
				attr_type = 's'
			# convert items to integers, if type specifies this
			if attr_type == 'd':
				try:
					value = int(value) 
				except:
					aprint("ERROR: expected integer value for option '%s'" % name)
					sys.exit(1)
			setattr(config, name, value)
		else:
			# this is the start of a multi-line block
			vstart = value.find('"""')
			block = value[vstart+3:] + '\n'
			in_block = 1
			# sanity check for block terminator on same line
			# if triple-quotes end this line, then block begins
			# and ends on the same line.
			if block.endswith('"""\n'):
				value = block[:-3]
				setattr(config, name, value)
				in_block = 0

	return config

# search for config file, using the following priority:
#  one specified on command line
#  one it /etc (system-wide)
#  one in ~/ (user-specific)
# if none found, return ""
def find_config_file(user_specified_config_file):
	# FIXTHIS - should check for config file readable, in find_config_file
	if user_specified_config_file:
		# check that file exists
		if not os.path.isfile(user_specified_config_file):
			aprint("ERROR: config file '%s' not found" % \
				user_specified_config_file)
			aprint("Exiting.")
			sys.exit(1)
		else:
			return user_specified_config_file

	# check for system sneaksync.conf file
	if os.path.isfile("/etc/sneaksync.conf"):
		return "/etc/sneaksync.conf"

	# check for user-specific ~/.sneaksync.conf file
	home_dir = ""
	try:
		home_dir = os.environ["HOME"]
	except:
		pass

	if home_dir and os.path.isfile(home_dir+os.path.sep+".sneaksync.conf"):
		return home_dir+os.path.sep+".sneaksync.conf"

	# nothing found...
	return ""

def get_config(default_config, config_path, mode, command_line_re_list):
	if config_path:
		config = read_config_file(config_path)
	else:
		config = default_config
		aprint("WARNING: using test config settings....")
		aprint("   please specify a config file using -c, or place")
		aprint("   a config file in '/etc' or your home directory.")

	# NOTE - source index does not need to be in the sync dir
	# but it makes it easier to debug the script.
	# source_index should NOT be inside the source_dir, however
	# FIXTHIS - should probably make source_index path configurable
	config.source_index = config.sync_dir+os.path.sep+"source_index"
	config.dest_index = config.sync_dir+os.path.sep+"dest_index"
	config.transfer_prefix = "transfer_file_"
	config.transfer_format = config.sync_dir+os.path.sep+ \
		config.transfer_prefix+"%05d"

	# directories should be absolute paths
	if mode=="send":
		config.source_dir = os.path.abspath(config.source_dir)
	else:
		config.dest_dir = os.path.abspath(config.dest_dir)

	config.exclude_re_list = []
	exclude_list = config.exclude_list.split("\n")
	exclude_list.extend(command_line_re_list)
	try:
		for exclude_line in exclude_list:
			exclude_line = exclude_line.strip()
			if not exclude_line:
				continue
			re_pat = re.compile(exclude_line)
			config.exclude_re_list.append(re_pat)
	except:
		aprint("ERROR: could not process exclude list '%s'\n" % \
			config.exclude_list)
	
	return config

# mode should be "send" or "receive"
def validate_config(config, mode):
	dprint("in validate_config(): config=%s" % config.__dict__)

	# do some sanity checks on the config
	if not os.path.isdir(config.sync_dir):
		aprint("ERROR: sync dir '%s' is not a directory" \
			 % config.sync_dir)
		aprint("Exiting.")
		sys.exit(1)

	if mode == "send":
		if not os.path.isdir(config.source_dir):
			aprint("ERROR: source dir '%s' is not a directory" \
				 % config.source_dir)
			aprint("Exiting.")
			sys.exit(1)

	if mode == "receive":
		if not os.path.isdir(config.dest_dir):
			aprint("ERROR: source dir '%s' is not a directory" \
				 % config.dest_dir)
			aprint("Exiting.")
			sys.exit(1)


# define some classes for sync entries
# entries are generated either by scanning or by parsing
# when parsing, fields are set from strings in index entries
# when scanning, fields are set from file system and sha1 operations
class file_entry:
	def __init__(self, path, size, date, perms, base_path):
		# path is stored in filesystem (unescaped) format
		# path is relative to the base_path (not an absolute path)
		self.path = path
		self.size = size
		self.date = date
		self.perms = perms
		self.base_path = base_path
		self.start_offset = 0
		self.end_offset = size
		self.last_offset = size
		self.sha1 = "<none>"
		self.sub_entries = []
		self.parent = None
		self.is_dir = False
	
		# technically true, but use this since this is not a part
		self.is_last_part = False
		self.is_part = False
	
	def fs_path(self):
		return self.base_path + os.path.sep + self.path

	def escape_path(self, path):
		# escape sequences are:
		# % -> %25
		# , -> %2C
		new1 = path.replace("%", "%25")
		new2 = new1.replace(",", "%2C")
		return new2

	def unescape_path(self, path):
		# back these out in reverse order of application
		new1 = path.replace("%2C", ",")
		new2 = new1.replace("%25", "%")
		return new2

	def mtime(self):
		# returns time in seconds since epoch
		(date_part,time_part) = self.date.split("_")
		year_str, month_str, day_str = date_part.split("-")
		hour_str, minute_str, second_str = time_part.split(":")
		d = datetime.datetime(int(year_str), int(month_str),
			int(day_str), int(hour_str), int(minute_str),
			int(second_str))
		return time.mktime(d.timetuple())

	def calculate_sha1(self):
		global index_cache

		# check the cache first
		if index_cache:
			dprint("in calculate_sha1, have index_cache")
			prev = index_cache.lookup_by_path(self.path)
			dprint("prev=%s" % prev)
			if prev and self.size == prev.size and \
				self.date == prev.date and \
				self.perms == prev.perms:
				dprint("item in cache, using previous sha1 of %s" % \
					prev.sha1)	
				self.sha1 = prev.sha1
			else:
				dprint("no match found in cache")

			if self.sha1 != "<none>":
				return self.sha1
			# otherwise, drop through to calculating it from the
			# file content

		# for a file or part, get sha1 of (part of) the file content
		try:
			f = open(self.fs_path(), 'rb')
		except IOError:
			# file may be in use, just skip it for now
			self.sha1 = 0
			return self.sha1

		offset = self.start_offset
		f.seek(offset)
		d = sha.new()
		buf = 0
		while buf != b'' and offset < self.end_offset:
			# only read to end_offset (not into the next part)
			amount_to_read = min(32768, self.end_offset - offset)
			buf = f.read(amount_to_read)
			d.update(buf)
			offset += len(buf)

		f.close()
		self.sha1 = d.hexdigest()
		return self.sha1

	def base_line(self):
		epath = self.escape_path(self.path)
		# use posix separator ("/") in index and transfer entries
		if os.path.sep != posix_sep:
			epath = epath.replace(os.path.sep, posix_sep)
		if self.sha1 == "<none>":
			aprint("ERROR: sha1 not calculated yet!")
		return "%s, %d, %s, %s, %s" % (epath, self.size,
			self.date, self.perms, self.sha1)

	def to_line(self):
		return "f, "+self.base_line()

	def attr_match(self, entry):
		dprint("self.sha1=%s" % self.sha1)
		dprint("entry.sha1=%s" % entry.sha1)
		dprint("self.size=%d" % self.size)
		dprint("entry.size=%d" % entry.size)
		dprint("self.date=%s" % self.date)
		dprint("entry.date=%s" % entry.date)
		dprint("self.perms=%s" % self.perms)
		dprint("entry.perms=%s" % entry.perms)
		if self.sha1 == entry.sha1 and \
			self.size == entry.size and \
			self.date == entry.date and \
			self.perms == entry.perms:
			dprint("matched!")
			return self
		else:
			dprint("did not match!")
			return None
	def __repr__(self):
		return "entry %s" % os.path.basename(self.path)

	def add_sub_entry(self, entry):
		self.sub_entries.append(entry)
		entry.parent = self

	def sort_sub_entries(self):
		entry_list = sorted(self.sub_entries,
			key = lambda entry: entry.path)
		self.sub_entries = entry_list

	def lookup_by_path(self, path):
		dprint("in lookup_by_path()")
		dprint("self=%s" % self)
		dprint("path=%s" % path)
		if self.path == path:
			return self
		else:
			found = None
			for child in self.sub_entries:
				if path.startswith(child.path):
					found = child.lookup_by_path(path)
					if found:
						break
			return found


class dir_entry(file_entry):
	def __init__(self, path, size, date, perms, base_path):
		file_entry.__init__(self, path, size, date, perms, base_path)
		self.is_dir = True

	def calculate_sha1(self):
		# sha1 for a directory is the digest of sub-entry strings
		# sorted by path
		self.sort_sub_entries()
		temp = ""
		for entry in self.sub_entries:
			temp += entry.to_line()+"\n"

		# now generate the digest from the buffer
		self.sha1 = sha.new(temp).hexdigest()
		return self.sha1

	def to_line(self):
		return "d, "+self.base_line()


# path for file ends in ".partyyofxx"
# e.g.  testfile.part02of13
#               01234567890
class part_entry(file_entry):
	def __init__(self, path, size, date, perms, base_path):
		file_entry.__init__(self, path, size, date, perms, base_path)
		# NOTE: self.path has the filename including the .partyyofxx extension

		dprint("Creating part for file '%s'" % path)
		self.is_last_part = False

		# check for and parse partyyofxx
		if len(path)<12:
			aprint("ERROR: invalid path '%s' for part_entry class\n", path)
			part_desc = ".part00of99"
		else:
			part_desc = path[-11:]

		pat = "[.]part[0-9][0-9]of[0-9][0-9]"
		if not re.match(pat, part_desc):
			aprint("ERROR: invalid path %s for part_entry class\n");
		part_num = int(part_desc[5:7])
		part_max = int(part_desc[9:11])
		dprint("part_num=%d" % part_num)
		dprint("part_max=%d" % part_max)
		dprint("size=%d" % size)

		# real_path has the path to the filename this is a part of
		self.real_path = path[:-11]
		self.start_offset = (part_num-1) * config.part_size
		self.end_offset = part_num * config.part_size

		# guess the length of the whole file
		self.last_offset = (part_max-1) * config.part_size + 1

		# need to adjust end_offset of last piece 
		# if creating part_entry from entry line, size will be
		#    the size of the part, and size specified will be smaller 
		#    than the part_size
		# if creating part_entry from a file, size will be the
		#    size of the whole file (> than part_size)
		if part_num == part_max:
			self.is_last_part = True
			if size <= config.part_size:
				self.end_offset = self.start_offset + size
			else:
				self.end_offset = size

			self.last_offset = self.end_offset

		dprint("start_offset=%d" % self.start_offset)
		dprint("end_offset=%d" % self.end_offset)
		dprint("last_offset=%d" % self.last_offset)

		self.size = self.end_offset - self.start_offset
		dprint("self.size=%d" % self.size)
		self.sha1 = "<none>"
		self.is_part = True

	def fs_path(self):
		# return path to file this part is part of
		return self.base_path + os.path.sep + self.real_path
	
	def to_line(self):
		return "p, "+self.base_line()


class index_class:
	def __init__(self, sequence, root):
		self.sequence = sequence
		self.root = root

def parse_index_entry(line, base_path):
	parts = line.split(", ")
	if len(parts) != 6:
		aprint("!!! ERROR - problem extracting index data")
		aprint("line was: '%s'" % line)
	entry_type = parts[0]
	path = parts[1]
	size = int(parts[2])
	date = parts[3]
	perms = parts[4]
	sha1 = parts[5]

	# convert from posix to local path separators, if different
	# FIXTHIS - can't sync paths with backslashes from Linux to Windows
	if os.path.sep != posix_sep:
		path = path.replace(posix_sep, os.path.sep)

	# VERY IMPORTANT!!
	# sanity-check the input here.  Path must be inside
	# the intended base directory, or we reject it.
	# This is to prevent tampered-with transfer files
	# touching stuff they are not supposed to.
	test_path = base_path + os.path.sep + path
	if not os.path.realpath(test_path).startswith(os.path.realpath(base_path)):
		aprint("ERROR: path '%s' is not inside base path '%s'" % \
			(path, base_path))
		aprint("  Aborting program...")
		raise ValueError
	
	if entry_type == "f":
		e = file_entry(path, size, date, perms, base_path)
	elif entry_type == "p":
		e = part_entry(path, size, date, perms, base_path)
	else:
		e = dir_entry(path, size, date, perms, base_path)
	e.path = e.unescape_path(path)
	e.sha1 = sha1
	return e

def perm_to_ascii(mode):
	return "perm-not-done"

def relative_path(full_path, base_path):
	# trim path to be relative to base_path
	if not full_path.startswith(base_path):
		aprint("WARNING: path '%s' is not inside base path of '%s'" \
			% (full_path, base_path))
		aprint("cannot create relative path")
		return full_path
	
	rel_path = full_path[len(base_path)+1:]
	return rel_path

def create_file_entry(full_path, base_path):
	st = os.stat(full_path)

	rel_path = relative_path(full_path, base_path)
	
	size = st.st_size
	date = datetime.datetime.fromtimestamp(st.st_mtime)
	date_str = date.strftime("%Y-%m-%d_%H:%M:%S")
	perm_str = perm_to_ascii(stat.S_IMODE(st.st_mode))

	entry = file_entry(rel_path, size, date_str, perm_str, base_path)
	entry.calculate_sha1()
	return entry

def create_dir_entry(full_path, base_path):
	st = os.stat(full_path)

	rel_path = relative_path(full_path, base_path)

  	# Ignore size for dirs.
	# size may be different, even for directories with identical contents.
	# IDEA: could use count of contents as size instead in create_dir_entry
	size = 0

	date = datetime.datetime.fromtimestamp(st.st_mtime)
	date_str = date.strftime("%Y-%m-%d_%H:%M:%S")
	perm_str = perm_to_ascii(stat.S_IMODE(st.st_mode))

	entry = dir_entry(rel_path, size, date_str, perm_str, base_path)
	return entry

def create_part_entries(fentry, base_path):
	entries = []

	path = fentry.path
	size = fentry.size
	date = fentry.date
	perms = fentry.perms
	
	part_max = size / config.part_size
	if size % config.part_size > 0:
		part_max += 1
	part_num = 1
	while part_num <= part_max:
		part_desc = ".part%02dof%02d" % (part_num, part_max)
		part_path = path+part_desc
		dprint("in create_part_entries, size=%d" % size)
		entry = part_entry(part_path, size, date, perms, base_path)
		entry.calculate_sha1()
		entries.append(entry)
		part_num += 1
	return entries


# returns a list containing: 1) a single file, 2) a single directory, or
#    3) a file and multiple part entries
def process_path(cur_path, base_path):
	global data

	show_activity()

	dprint("Processing path '%s'" % cur_path)
	full_path = os.path.abspath(cur_path)

	# ignore items that match any regex pattern in the exclude list
	for re_pat in config.exclude_re_list:
		if re_pat.search(cur_path):
			dprint("Omitting path '%s' due to exclude pattern '%s'\n" % (cur_path, re_pat.pattern))
			return None

	# if it's a regular file, just create the node
	if os.path.isfile(full_path):
		# if file is a part, put it on a special list
		# This is used later to force a re-caculation of
		# the sha1 for the base file (and its parts)
		pat = ".*[.]part[0-9][0-9]of[0-9][0-9]$"
		if re.match(pat, full_path):
			# remove this file
			# comment this out to keep file for debugging parts
			os.remove(full_path)
			# force a recalc on it's base file
			if not full_path.startswith(base_path):
				aprint("ERROR: can't truncate full_path %s safely\n" % full_path)

			rel_path = full_path[len(base_path)+1:-11]
			dprint("Adding %s to recalc_list" % rel_path)
			data.recalc_list.append(rel_path)
			# and don't put this (the part) in the index
			return None
		else:
			entry = create_file_entry(full_path, base_path)
			entries = [entry]
			if entry.size > config.part_size:
				part_entries = create_part_entries(entry, base_path)
				entries.extend(part_entries)
			return entries
	
	# if a directory, create the node and populate children before
	# calculating the sha1
	if os.path.isdir(full_path):
		entry = create_dir_entry(full_path, base_path)

		try:
			item_list = os.listdir(full_path)
		except WindowsError:
			# ignore junction directories
			# FIXTHIS - do I need to not create the dir entry also?
			item_list = ""

		for item in item_list:
			item_path = os.path.join(full_path, item)
			
			child_list = process_path(item_path, base_path)
			if child_list:
				for child in child_list:
					entry.add_sub_entry(child)

		entry.calculate_sha1()
		return [entry]
			
	# skip things that are not files or directories
	# (symlinks, device files, etc.)
	return None

def print_tree(entry):
	aprint(entry.to_line())
	if entry.sub_entries:
		# NOTE: sub_entries should already be sorted
		for child in entry.sub_entries:
			print_tree(child)

def write_tree(f, entry):
	f.write(entry.to_line()+"\n")
	if entry.sub_entries:
		# NOTE: sub_entries should already be sorted
		for child in entry.sub_entries:
			write_tree(f,child)

def write_index(filename, index):
	f = open(filename, "wb")

	seq_line = "%040d\n" % index.sequence
	f.write(seq_line)

	# now write the tree
	write_tree(f, index.root)
	f.close()

# recalculate items in the index tree, based on the recalc_list
# during processing, some items may have picked up wrong sha1 values
# from the index cache.  This routine corrects those items in the index
# so it doesn't get written out with bad data.

# this routine can either just invalidate the sha1 (used on an index_cache),
# or do a real sha1 re-calculation (used on the real index)

def process_recalc_list(entry, do_recalc):
	global data
	global index_cache

	# don't use the cache for any recalculations done
	saved_cache = index_cache
	index_cache = None

	dprint("Checking for items that need sha1 recalculation...")
	dprint("recalc_list=%s" % data.recalc_list)
	for recalc_path in data.recalc_list:
		dprint("found path %s on recalc_list" % recalc_path)
		recalc_entry = entry.lookup_by_path(recalc_path)
		dprint("looking up entry for recalc - got %s" % recalc_entry)
		if recalc_entry:
			dprint("recalculating sha1 for %s" % recalc_entry.path)
			if do_recalc:
				recalc_entry.calculate_sha1()
			else:
				recalc_entry.sha1 = "<none>"

			# recalc on parts of a file as well
			if not recalc_entry.is_dir:
				for child in recalc_entry.sub_entries:
					if do_recalc:
						recalc_entry.calculate_sha1()
					else:
						recalc_entry.sha1 = "<none>"

			# recalculate my parents up the tree
			# note: some nodes may be recalculated multiple times
			p = recalc_entry.parent
			while p:
				if do_recalc:
					p.calculate_sha1()
				else:
					p.sha1 = "<none>"
				p = p.parent

	data.recalc_list = []
	index_cache = saved_cache

		
def generate_index(seq, filename, dir_to_be_indexed):
	global index_cache

	# scan directory, generating index lines
	entry_list = process_path(dir_to_be_indexed, dir_to_be_indexed)

	# sanity check - should return a list containing a single dir entry
	if not entry_list or len(entry_list)>1 or not entry_list[0].is_dir:
		aprint("ERROR: unexpected result from process_path")
	entry = entry_list[0]

	# recalculate sha1s for any entries that may need it
	process_recalc_list(entry, True)

	# use specified sequence
	index = index_class(seq, entry)

	write_index(filename, index)

# returns sequence of index file
def read_index_seq(filename):
	f = open(filename, "rb")

	seq_line = f.readline()
	sequence = int(seq_line)
	f.close()
	return sequence

# returns an index object for the root of the tree
def read_index(filename, base_path):
	f = open(filename, "rb")

	seq_line = f.readline()
	sequence = int(seq_line)

	root = None
	dir_stack = []
	for line in f.xreadlines():
		# parse line with trailing whitespace ('\n') removed
		entry = parse_index_entry(line.rstrip(), base_path)
		dprint("processing entry %s" % entry.path)
		dprint("dir_stack=%s" % dir_stack)

		if not root:
			# this only happens once
			dir_stack.append(entry)
			root = entry
			continue

		# if I'm a part, put me on my file's child list
		if entry.is_part:
			base_entry = root.lookup_by_path(entry.real_path)
			if not base_entry:
				aprint("ERROR: file entry for part is missing.")
				aprint("entry.path=%s" % entry.path)
				continue

			base_entry.add_sub_entry(entry)
			continue

		# manage dir_stack
		# pop items that are not my parent directories
		while not entry.path.startswith(dir_stack[-1].path):
			dir_stack.pop()

		# put me on my parent dir's child list
		if entry.path.startswith(dir_stack[-1].path):
			dir_stack[-1].add_sub_entry(entry)
		else:
			aprint("ERROR: parent dir missing from dir_stack")
			aprint("entry.path=%s" % entry.path)
			aprint("dir_stack[-1].path=%s" % dir_stack[-1].path)

		# if I'm a directory, put me on dir_stack
		if entry.is_dir:
			dir_stack.append(entry)
	
	return index_class(sequence, root)

def update_index(index_path, dir_to_be_indexed):
	global index_cache

	new_index_path = index_path+".new"
	# if file doesn't exist, create it
	if not os.path.isfile(index_path):
		index_cache = None
		seq = 0
	else:
		# read the old index to use as a sha1 cache
		prev_index = read_index(index_path, dir_to_be_indexed)
		index_cache = prev_index.root
		seq = prev_index.sequence

	# remove sha1s for anything on the recalc_list
	process_recalc_list(index_cache, False)

	show_activity()

	# new version of index should have next sequence num
	generate_index(seq+1, new_index_path, dir_to_be_indexed)
	index_cache = None

	end_activity()

	try:
		os.rename(new_index_path, index_path)
	except:
		# on Windows, if the file exists, the rename may give
		# an exception.  In this case, fall back to remove, then rename
		try:
			os.remove(index_path)
			os.rename(new_index_path, index_path)
		except:
			aprint("ERROR trying to atomically rename index file")
			aprint("Can't rename '%s' to '%s'" % \
				(new_index_path, index_path))
	

def get_transfer_file_list():
	global data
	global config

	# if we haven't scanned for the transfer files, do that now.
	if not data.transfer_file_list_scanned:
		# scan sync_dir looking for transfer files
		file_list = os.listdir(config.sync_dir)

		tfiles = []
		for filename in file_list:
			if filename.startswith(config.transfer_prefix):
				tfiles.append(config.sync_dir+os.path.sep+filename)
		data.transfer_file_list = tfiles
		data.transfer_file_list_scanned = True

	dprint("transfer files=%s" % data.transfer_file_list)
	return data.transfer_file_list

def get_next_transfer_filename():
	global data
	global config

	# if we haven't scanned for max_transfer_num, do that now.
	if data.max_transfer_num == -1:
		tfiles = get_transfer_file_list()

		# get largest number used so far
		max_num = 0
		for filename in tfiles:
			try:
				end_num = int(filename[-5:])
				if end_num > max_num:
					max_num = end_num
			except:
				pass
		data.max_transfer_num = max_num

	data.max_transfer_num += 1
	filename = config.transfer_format % data.max_transfer_num
	dprint("next transfer filename='%s'" % filename)
	return filename

# transfer_type must be one of: "remove" or "update"
def make_transfer_file(entry, transfer_type):
	global data

	filename = get_next_transfer_filename()
	vprint("Creating transfer file '%s' for file '%s'" % \
		(filename, entry.path))

	outfile = open(filename, "wb")
	data.transfer_file_list.append(filename)

	if transfer_type == "remove":
		# this is a "remove" transfer
		saved_sha1 = entry.sha1
		entry.sha1 = "<deleted>"
		outfile.write(entry.to_line()+"\n")
		entry.sha1 = saved_sha1
		outfile.close()
		data.transfer_count += 1
		data.transfer_file_total_size += os.path.getsize(filename)
		return

	# must be an "update" transfer
	outfile.write(entry.to_line()+"\n")

	if not entry.is_dir:
		# put file content after entry line
		# content is from entry.start_offset to entry.end_offset
		dprint("trying to write %d bytes for file %s" % \
			(entry.size, entry.fs_path()))
		try:
			infile = open(entry.fs_path(), "rb")
		except:
			# some kind of error, just skip this file
			aprint("ERROR: can not read data for file '%s'" % \
				entry.fs_path())
			outfile.close()
			data.transfer_count += 1
			data.transfer_file_total_size += os.path.getsize(filename)
			return
			
		offset = entry.start_offset
		infile.seek(offset)
		buf = 0
		while buf != b'' and offset < entry.end_offset:
			amount_to_read = min(32768, entry.end_offset - offset)
			buf = infile.read(amount_to_read)
			outfile.write(buf)
			offset += len(buf)
		infile.close()

	outfile.close()
	data.transfer_count += 1
	data.transfer_file_total_size += os.path.getsize(filename)

def get_current_transfer_size():
	global data

	if not data.transfer_file_list_scanned:
		# scan list of transfer files, and sum their sizes
		tfiles = get_transfer_file_list()

		total_size = 0
		for filename in tfiles:
			total_size += os.path.getsize(filename)
		data.transfer_file_total_size = total_size

	dprint("current total transfer size=%d" % \
		data.transfer_file_total_size)
	return data.transfer_file_total_size

def get_current_transfer_count():
	# count the list of transfer files
	tfiles = get_transfer_file_list()

	count = len(tfiles)
	dprint("current transfer count=%d" % count)
	return count


def within_transfer_limit(entry):
	global config
	global data

	# check count and size limits for transfer directory
	if get_current_transfer_count() >= config.transfer_count_limit:
		if data.transfer_skip_count < 20:
			vprint("  Too many files already transfered, not sending '%s' this time" % \
				entry.path)
		else:
			vvprint("  Too many files already transfered, not sending '%s' this time" % \
				entry.path)
		data.transfer_skip_count += 1
		return False
	elif get_current_transfer_size() + entry.size >= \
		config.transfer_size_limit:
		if data.transfer_skip_count < 20:
			vprint("  Not enough space left, not sending '%s' this time" % \
				entry.path)
		else:
			vvprint("  Not enough space left, not sending '%s' this time" % \
				entry.path)
		data.transfer_skip_count += 1
		return False
	elif entry.size > config.part_size:
		vvprint("file size greater than part limit, not sending '%s'" % \
			entry.path)
		if entry.is_part:
			aprint("ERROR: Entry part '%s' needing transfer is not within %d part size limit" % \
				(entry.path, config.part_size))
		# NOTE: don't count this in our skip_count
		return False
	else:
		return True

######################## new action system ############################
def walk_tree_for_updates(entry, other_tree):
	# look up entry in other tree, if different, do callback
	dprint("walking tree at entry %s" % entry)
	other = other_tree.lookup_by_path(entry.path)
	dprint("other= %s" % other)

	# if item is not found, or is different in other tree, update it
	if other == None or not other.attr_match(entry):
		if within_transfer_limit(entry):
			make_transfer_file(entry, "update")

	if entry.sub_entries:
		dprint("%s is a dir, walking children" % entry.path)
		for child in entry.sub_entries:
			walk_tree_for_updates(child, other_tree)

def walk_tree_for_removes(entry, other_tree):
	# look up entry in other tree, if different, do callback
	dprint("walking tree at entry %s" % entry)
	other = other_tree.lookup_by_path(entry.path)
	dprint("other= %s" % other)

	# if item is not present in other tree, remove it
	if other == None:
		make_transfer_file(entry, "remove")

	if entry.sub_entries:
		dprint("%s is a dir, walking children" % entry.path)
		for child in entry.sub_entries:
			walk_tree_for_removes(child, other_tree)

def do_send():
	global config

	# check that dest_index is present
	if not os.path.isfile(config.dest_index):
		prog_name = os.path.basename(sys.argv[0])
		aprint("ERROR: could not find dest_index: '%s'" \
			% config.dest_index)
		aprint("Cannot continue in send mode.")
		aprint("Create dest_index file by running '%s -r' on the destionation host" % prog_name)
		aprint("Exiting.")
		sys.exit(1)

	# for DEBUGGING - receiver can leave transfer files around
	# get rid of them before starting
#	tfiles = get_transfer_file_list()
#	for filename in tfiles:
#		os.remove(filename)

	start_activity("Updating source index")
	update_index(config.source_index, config.source_dir)

	# read the source and destination indexes
	vprint("Reading source index")
	si = read_index(config.source_index, config.source_dir)

	# NOTE: use source_dir as base path for dest_index
	# - this is not right, but we're not going to write anything to it
	vprint("Reading destination index")
	di = read_index(config.dest_index, config.source_dir)
	
	# scan the indexes, and print a message for each difference
	# do a traversal, and look up the item in the other tree
	vprint("Performing update actions")
	walk_tree_for_updates(si.root, di.root)
	
	vprint("Performing removal actions")
	walk_tree_for_removes(di.root, si.root)

def process_transfer_file(filename):
	global data

	# read transfer file and act on it
	tf = open(filename, "rb")
	line = tf.readline()
	entry = parse_index_entry(line.rstrip(), config.dest_dir)

	vprint("Processing transfer file %s, for path '%s'" \
		% (filename[-5:], entry.path))

	# ignore items that match any regex pattern in the exclude list
	for re_pat in config.exclude_re_list:
		if re_pat.search(entry.path):
			dprint("Omitting path '%s' due to exclude pattern '%s'\n" % (cur_path, re_pat.pattern))
			tf.close()
			dprint("Removing transfer file")
			os.remove(filename)
			data.transfer_count += 1
			return

	# NOTE - transfer file is still open at this point

	if entry.sha1 == "<deleted>":
		tf.close()
		if entry.is_dir:
			vprint("Deleting directory")
			try:
				os.rmdir(entry.fs_path())
			except:
				pass
			dprint("os.rmdir('%s')" % entry.path)
		else:
			vprint("Deleting file")
			try:
				os.remove(entry.fs_path())
			except:
				pass
			dprint("os.remove('%s')" % entry.path)

		# FIXTHIS -what about removing parts? (not sure it makes sense)

		dprint("Removing transfer file")
		os.remove(filename)
		data.transfer_count += 1
		return

	# not a remove...
	# don't let old sha1 get used
	data.recalc_list.append(entry.path)

	# Now, let's make something
	if entry.is_dir:
		# make directory, if not already present
		vprint("Creating directory")
		tf.close()
		try:
			os.makedirs(entry.fs_path())
		except:
			pass
	else:
		vprint("Copying file contents")
		# create leading dirs in destination dir if needed
		try:
			os.makedirs(os.path.dirname(entry.fs_path()))
		except:
			pass

		# if file doesn't exist yet, create it
		if not os.path.exists(entry.fs_path()):
			try:
				open(entry.fs_path(), "wb").close()
			except: 
				aprint("ERROR creating file '%s'" % entry.fs_path())
				return

		# copy the file contents from remainder of transfer file
		dprint("writing to file %s" % entry.fs_path())
		# NOTE - don't mess with this open mode unless you know
		# exactly what you're doing...  Other modes truncate the file
		# or don't open newly created files.
		outfile = open(entry.fs_path(), "r+b")
		offset = entry.start_offset
		dprint("seeking to offset %d" % offset)
		outfile.seek(offset)
		dprint("tell offset is %d" % outfile.tell())
		buf = 0
		while buf != b'' and offset < entry.end_offset:
			amount_to_read = min(32768, entry.end_offset - offset)
			dprint("reading %d bytes" % amount_to_read)
			buf = tf.read(amount_to_read)
			dprint("writing %d bytes ('%s') at offset %d" % (len(buf), "buf", offset))
			outfile.write(buf)
			offset += len(buf)
			dprint("after write, my offset is %d" % offset)
			dprint("tell offset is %d" % outfile.tell())

		# if entry is a part, pad the file to be close to correct size
		# this is needed to force the dest_index to have the correct
		# number of parts for large files
		if entry.is_part:
			dprint("entry.is_part=True")
			if not entry.is_last_part:
				dprint("writing byte at last_offset %d" % \
					(entry.last_offset))
				outfile.seek(entry.last_offset)
				outfile.write('$')

			outfile.close()
			
			# write the part file itself also
			tf.seek(0)
			line = tf.readline()
			part_path = entry.base_path + os.path.sep + entry.path
			outfile = open(part_path, "wb")
			buf = 0
			while buf != b'':
				buf = tf.read(32768)
				outfile.write(buf)

		outfile.close()

		# if dest file is too big (e.g. it was shortened on source),
		# truncate it
		if os.path.getsize(entry.fs_path()) > entry.last_offset:
			outfile = open(entry.fs_path(), "r+b")
			outfile.truncate(entry.last_offset)

		tf.close()

	# touch the file date and time
	mtime = entry.mtime()
	os.utime(entry.fs_path(), (mtime,mtime))
	
	# FIXTHIS - write code to adjust permissions

	dprint("Removing transfer file")
	os.remove(filename)

	data.transfer_count += 1

def do_receive():
	global config

	# process any transfer files
	tfiles = get_transfer_file_list()

	# it is important for part entries to process them in order
	tfiles.sort()

	for filename in tfiles:
		process_transfer_file(filename)

	# update destination index
	start_activity("Updating dest_index file")
	update_index(config.dest_index, config.dest_dir)

ESC = '\x1B'
class tty_data_class:
	def __init__(self):
		self.show_activity = False
		self.backspace = '\x08'
		self.erase_end_of_line = ESC+'[K'
		self.erase_cur_line = ESC+'[2K'
		self.erase_screen = ESC+'[2J'
		self.beginning_of_line = ESC+'[0G'
		self.rotate_char_list = ['|','/','-','\\']
		self.rotate_index = 0
		self.msg_string = "activity: %s"

	def put_msg_and_char(self, rchar):
		# FIXTHIS - could put activity spinner at specific location
		# on screen (saved in start_activity?)
		sys.stdout.write(self.beginning_of_line)
		sys.stdout.flush()
	#	sys.stdout.write(self.erase_cur_line)
	#	sys.stdout.flush()
	#	sys.stdout.write(self.beginning_of_line)
	#	sys.stdout.flush()
		sys.stdout.write(self.msg_string % rchar)
		sys.stdout.flush()

	def put_char(self, rchar):
		# backup and rewrite one char
		sys.stdout.write(self.backspace+rchar)
		sys.stdout.flush()


tty_data = tty_data_class()

# update an activity spinner

def start_activity(msg):
	tty_data.msg_string = msg + " %s"
	if not tty_data.show_activity:
		vprint(msg)
		return

	tty_data.put_msg_and_char('|')

def show_activity():
	global tty_data

	if not tty_data.show_activity:
		return

	# figure out next char to write
	rindex = tty_data.rotate_index+1
	if rindex >= len(tty_data.rotate_char_list):
		rindex = 0
	rchar = tty_data.rotate_char_list[rindex]
	tty_data.rotate_index = rindex

	tty_data.put_char(rchar)

def end_activity():
	if not tty_data.show_activity:
		return

	tty_data.put_char('.')
	print
	sys.stdout.flush()

def main():
	global debug
	global verbose
	global default_config
	global config
	global data
	global log_file
	global index_cache

	mode = "send"
	do_gen_index = False
	user_specified_config_file = ""
	log_file = None
	period = 0
	re_list = []

	if "-r" in sys.argv:
		sys.argv.remove("-r")
		mode = "receive"

	if "-h" in sys.argv or "--help" in sys.argv:
		usage()
		sys.exit(0)

	if "-v" in sys.argv:
		sys.argv.remove("-v")
		verbose = 1

	if "-vv" in sys.argv:
		sys.argv.remove("-vv")
		verbose = 2

	if "--debug" in sys.argv:
		sys.argv.remove("--debug")
		debug = 1

	if "-i" in sys.argv:
		sys.argv.remove("-i")
		do_gen_index = True

	if "-c" in sys.argv:
		index = sys.argv.index("-c")
		user_specified_config_file = sys.argv[index+1]
		vprint("Using config file: %s" % user_specified_config_file)
		del sys.argv[index+1]
		del sys.argv[index]

	if "-l" in sys.argv:
		index = sys.argv.index("-l")
		log_file = sys.argv[index+1]
		del sys.argv[index+1]
		del sys.argv[index]

	if "-p" in sys.argv:
		index = sys.argv.index("-p")
		period_str = sys.argv[index+1]
		del sys.argv[index+1]
		del sys.argv[index]
		try:
			period = int(period_str)
		except:
			aprint("ERROR: problem parsing period argument: '%s'" \
				% period_str)
			aprint("Exiting.")
			sys.exit(1)

	while "-x" in sys.argv:
		index = sys.argv.index("-x")
		re_item = sys.argv[index+1]
		del sys.argv[index+1]
		del sys.argv[index]
		re_list.append(re_item)

	if "-a" in sys.argv:
		sys.argv.remove("-a")
		tty_data.show_activity = True
	
	if "--version" in sys.argv:
		prog_name = os.path.basename(sys.argv[0])
		aprint("%s version %d.%d.%d" % \
			(prog_name, major_version, minor_version, revision))
		sys.exit(0)

	for arg in sys.argv[1:]:
		aprint("WARNING: unknown option '%s' was specified." % arg)
		aprint("  option is being ignored")

	if log_file:
		vprint("Using log file: %s" % log_file)
		open_log(log_file, mode, do_gen_index)

	# find, read and validate the configuration
	config_path = find_config_file(user_specified_config_file)
	config = get_config(default_config, config_path, mode, re_list)
	validate_config(config, mode)

	write_config_header_to_log(config, mode)

	# initialize global data (stats, counters, and transfer list)
	data = data_class()

	if do_gen_index:
		# remove transfer files
		for filename in get_transfer_file_list():
			os.remove(filename)


		# generate initial index
		index_cache = None
		if mode=="send":
			start_activity("Generating source index...")
			generate_index(1, config.source_index,
				config.source_dir)
		else:
			start_activity("Generating destination index...")
			generate_index(1, config.dest_index,
				config.dest_dir)

		end_activity()

		sys.exit(0)
		
	while True:
		# now actually do the send or receive
		if mode == "send":
			dest_seq = read_index_seq(config.dest_index)
			dprint("dest index sequence is %d\n" % dest_seq)
			do_send()
			aprint("Created %d transfer files." % data.transfer_count)
			if data.transfer_skip_count:
				aprint("Skipped %d items that still need to be transferred." % data.transfer_skip_count)
		else:
			do_receive()
			aprint("Processed %d transfer files." % data.transfer_count)

		close_log()

		if not period:
			break

		# re-initialize global data for next run
		data = data_class()
		index_cache = None

		# wait for next wakeup
		keep_waiting = True
		while keep_waiting:
			# FIXTHIS - should do logging of this activity
			vprint("Waiting for %d minutes..." % period)
			time.sleep(period*60)

			# poll for activity on other side
			# if a sender, check for dest_index sequence change
			# if a receiver, check for transfer files
			if mode == "send":
				vprint("Checking dest index sequence")
				new_dest_seq = read_index_seq(config.dest_index)
				vprint("New dest index sequence is %d" % \
					new_dest_seq)
				if new_dest_seq != dest_seq:
					keep_waiting = False
			else:
				vprint("Checking for transfer files")
				if get_transfer_file_list(): 
					keep_waiting = False

					# FIXTHIS - should wait for transfer
					# files to finish syncing
					# see todo.txt for notes on how
				else:
					# nothing found, rebuild list next time
					data.transfer_file_list_scanned = False

		# restart logging
		open_log(log_file, mode, do_gen_index)

	vprint("Done.")
	

if __name__ == "__main__":
	main()

# === Information about the data and algorithms for this program ===
# 
# the source machine has:
#   source_dir - the location of files to be backed up
#   source_index - the index of source_dir
#   sync_dir - the directory for transferring synced files
#     dest_index - the index of dest_dir
#     transfer_file_N - a file being transferred to the destination
#
# the destination machine has:
#   sync_dir - the directory for transferring synced files
#     dest_index - the index of dest_dir
#     transfer_file_N - a file being transferred to the destination
#   dest_dir - the location of backup files on the destination
#
# == sender algorithm ==
# each time run, do:
# update source_index
#   if source_index is not present, generate it
#   else scan source_dir, and update source_index
# transfer files to update
#   compare source_index with dest_index
#   if file is missing or different in dest_index
#     create transfer file in d1
#     loop until size would be exceeded or no more files to transfer
# watch dest_index for change 
# loop until there are no more files to transfer
#
# == receiver algorithm ==
# each time run, do:
# receive transfer files
#   if there's a transfer file,
#     [check if file in dest_dir matches or not] (optional optimization)
#     copy transfer file to dest_dir
#     update dest_index with entry for newly copied file
#       get sha1, rewrite dest_index
#     loop no more files to transfer
# update dest_index
#   if dest_index is not present, generate it
#   else scan dest_dir, and update dest_index
# check for more transfer files
# loop forever
#
# each index entry is one line with the format
# type, path, size, date, perms, sha1
# if path has commas, they are escaped as follows:
#   % are converted to %25
#   , are convereted %2C
# date is of format: 2014-12-31_12:59:59
# perms are unix perms in string format
#
# See NOTES-sneaksync.txt for more information

